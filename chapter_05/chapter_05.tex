% Adding a coloured vertical edge to the pages in the chapter
\ClearShipoutPicture
\AddToShipoutPicture{%
  \AtPageLowerLeft{%
    \checkoddpage
    \ifoddpage
      \begin{tikzpicture}[remember picture,overlay] % Odd page → right edge
        \draw[line width=80pt, colour_chapter5] 
             (\paperwidth,0) -- (\paperwidth,\paperheight);
      \end{tikzpicture}%
    \else
      \begin{tikzpicture}[remember picture,overlay] % Even page → left edge
        \draw[line width=80pt, colour_chapter5] 
             (0,0) -- (0,\paperheight);
      \end{tikzpicture}%
    \fi
  }%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Flash-flood-focused verification of rainfall-based 
predictions of areas at risk of flash floods}
\label{flash_flood_focused_verification_rainfall_based_ff}
\graphicspath{{chapter_05/figures}{chapter_05/tables}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\underline{\textbf{Authors' contribution for this chapter:}} Fatima M. Pillosu designed the study, with advice from Hannah Cloke and Christel Prudhomme, obtained the datasets, carried out the analysis, and led the writing of the manuscript. All authors assisted with writing the manuscript. Overall, 90\% of the writing was undertaken by Fatima M. Pillosu.

\vspace{\baselineskip}

\section*{PREFACE}
\addcontentsline{toc}{section}{PREFACE}

The first main analysis chapter (Chapter \ref{flash_flood_focused_verification_rainfall_based_ff}) undertakes the flash-flood-focused evaluation of short- and medium-range post-processed probabilistic point-scale rainfall forecasts. Previous research demonstrates that such forecasts predict localised extreme rainfall events more accurately than raw ERA5 outputs \citep{Pillosu_2025a}. Whilst improved rainfall prediction accuracy should enhance the identification of areas at risk of flash floods, a flash-flood-focused assessment remains essential to determine how effectively enhanced rainfall predictions translate into meaningful flash flood hazard identification. This chapter addresses \textcolor{colour_chapter5}{research question 1 (RQ1) "Can post-processed global NWP rainfall forecasts successfully identify areas at risk of flash floods up to medium-range lead times?"}. Additionally, this chapter fulfils the first research component of this thesis by \textcolor{colour_chapter5}{developing a flash-flood-focused verification framework for predictions of areas at risk of flash floods, designed to benchmark the capability of different predictive systems}. This evaluation establishes a performance benchmark against which more sophisticated modelling approaches - incorporating additional hydrological and topographical parameters - can be measured. Such comparative analysis determines whether the increased computational demands and data requirements of complex systems yield commensurate improvements in flash flood prediction accuracy, or whether simpler precipitation-based approaches provide sufficient utility for early warning applications.


\clearpage

\section*{ABSTRACT}
\addcontentsline{toc}{section}{ABSTRACT}

\clearpage



%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Flash floods cause significant societal, economic, and environmental impacts \citep{Dordevic_2020}. These events arise through two primary mechanisms. The first involves intense, short-duration rainfall events (typically less than 6 hours) that generate rapid surface runoff and channel response, particularly over steep slopes. The second occurs when sustained moderate-to-extreme rainfall over extended periods (> 6 hours) progressively saturates soils, reducing infiltration rates and promoting surface runoff that inundates low-lying areas \citep{Speight_2021}. Short-duration flash flood events typically result from small-scale convective systems, whilst prolonged events are more frequently associated with large-scale, mesoscale convective systems or hurricanes \citep{Doswell_2001}. The distinction between these mechanisms is essential as they exhibit different temporal scales, spatial patterns, and relationships with antecedent catchment conditions, and predictability limits. This study aims to enhance preparedness and mitigation strategies against this escalating threat, particularly in regions with limited resources and data, by assessing the effectiveness of global rainfall forecasts in identifying areas at risk of flash floods.

\begin{tcolorbox}[
  colframe=colour_chapter5, % Use the custom HEX colour for the vertical line
  colback=white,            % Background colour remains white
  sharp corners,            % Ensures sharp edges
  boxrule=2mm,              % Thickness of the left vertical line
  left=0mm,                 % Space inside the left margin
  right=0mm,                % No space on the right
  toprule=0mm,              % Remove the top border
  bottomrule=0mm,           % Remove the bottom border
  rightrule=2mm            % Remove the right border
]
{\color{colour_chapter5} {\setlength{\parindent}{1.0em} The term \textbf{"identification of areas at risk of flash floods"} refers to the \textit{prediction of polygons - or areas - that might experience flash flooding due to the expected rainfall}.}}
\end{tcolorbox}

Forecast-triggered mitigation strategies, such as early warning systems \citep{CoughlanDePerez_2022, ŠakićTrogrlić_2022} and forecast-based financing protocols \citep{Bischiniotis_2019a, Perez_2016}, have been shown to improve resilience, decrease mortality, and lower recovery costs against riverine floods. Yet, these strategies hinge on accurate, timely predictions. In lower-income countries, accurate forecasts with even longer lead times are required to set cost-effective mitigation strategies \citep{Bazo_2019, Kiptum_2023}. 

Over the years, flash flood forecasting systems have been developed at local/regional \citep{Speight_2018, Corral_2019, Ibarreche_2020, RamosFilho_2021, Shuvo_2021}, national \citep{Javelle_2016, Liu_2018}, and continental scales \citep{Gourley_2017, Raynaud_2015}, with varying degrees of model complexity and forecast accuracy. These systems share a reliance on high-density rainfall and discharge observations, and km-scale rainfall forecasts \citep{Braud_2014}. This approach has prevented the development of a flash flood forecasting system that provides medium-range forecasts over a continuous global domain. Radar-derived rainfall predictions remain limited to a few hours ahead \citep{Imhoff_2022}, and kilometre-scale forecasts show substantial skill reduction beyond two days \citep{Barrett_2019}. Moreover, the uneven spatial distribution of these data sources restricts flash flood prediction to a collection of separate regional and national systems, including WMO's "Flash Flood Guidance System with Global Coverage" \citep{Georgakakos_2022}. Consequently, many areas of the world remain without access to flash flood guidance, highlighting the need for alternative approaches to address the challenge of providing medium-range predictions of areas at risk of flash floods over a continuous global domain.

Global NWP models, such as ECMWF’s IFS, are increasingly seen as a viable solution to this challenge. These models provide daily global rainfall predictions up to medium-range leads but have historically struggled to accurately predict extreme localised rainfall events due to their coarse resolution and parametrisation schemes \citep{Emerton_2016, Wen_2021}. Owing to recent improvements in global NWP forecast accuracy \citep{Haiden_2023, Lavers_2021}, the interest in using them to provide flash flood guidance in data-scarce regions and extend predictions’ lead times has recently increased \citep{Bucherie_2022b}. Additionally, the development of state-of-the-art post-processing techniques can enhance the quality of raw forecasts and make them more suitable for flash flood prediction \citep{Vannitsem_2021}. For example, the ecPoint statistical post-processing technique transforms global grid-based forecasts into probabilistic point-scale predictions, improving the reliability and discrimination ability of rainfall forecasts up to day 10, especially for extremes \citep{Hewson_2021}.

Since a significant gap remains in leveraging global rainfall NWP forecasts for flash flood forecasting, this study aims to bridge this gap by evaluating the performance of ERA5-ecPoint rainfall forecasts in identifying areas at risk of flash floods. The CONUS, with its extensive collection of flash flood impact reports within NOAA's Storm Event Database and its varying climate, serves as an ideal test bed for this research. The research question posed in this study is \textit{can post-processed global NWP rainfall forecasts successfully identify areas at risk of flash floods up to medium-range lead times?} The innovation proposed by this research is twofold. It first proposes a flash-flood-focused verification framework for predictions of areas at risk of flash floods, in recognition of the non-linear relationship between flash floods and triggering rainfall events. Hence, this framework can be used as a common framework to benchmark the general capabilities of different systems to predict areas at risk of flash floods. This research also provides a performance benchmark against which more sophisticated modelling approaches - incorporating additional hydrological and topographical parameters - can be measured. Such comparative analysis is essential for determining whether the increased computational demands and data requirements of complex systems yield commensurate improvements in flash flood prediction accuracy, or whether simpler precipitation-based approaches provide sufficient utility for early warning applications.

% TO BE INCORPORATED IN THE INTRODUCTION: The application of the flash-flood-focused verification framework to the identification of areas at risk of flash floods involving solely rainfall predictions is done in the recognition that a robust understanding of rainfall forecast performance in identifying areas at risk of flash floods is essential to benchmark the performance of more advanced prediction methods, e.g. data-driven approaches involving more parameters.

This chapter is organised as follows. Sections \ref{flash_flood_focused_verification_rainfall_based_ff_DATA} and \ref{flash_flood_focused_verification_rainfall_based_ff_METHODS} describe the data and methods used to develop the flash-flood-focused verification framework. Section \ref{flash_flood_focused_verification_rainfall_based_ff_RESULTS} presents the results of the objective verification analysis, while section \ref{flash_flood_focused_verification_rainfall_based_ff_CASE_STUDY} presents results from a case-study-based subjective verification analysis. Section \ref{flash_flood_focused_verification_rainfall_based_ff_DISCUSSIONS} discusses the verification results, while section \ref{flash_flood_focused_verification_rainfall_based_ff_CONCLUSIONS} draws concluding remarks for the study.


%%%%%%%%%%%%%%
\section{Data}
\label{flash_flood_focused_verification_rainfall_based_ff_DATA}


\subsection{Study domain}

This research establishes the CONUS as the primary study domain (see Figure \ref{fig:conus_domain}). The CONUS domain presents diverse hydro-meteorological conditions, encompassing varied topographical features ranging from coastal plains to mountainous terrain, and climatic regimes spanning Mediterranean, continental, subtropical, and desert classifications. This diversity encompasses a broad spectrum of flash-flood-generating mechanisms, ranging from intense convective precipitation to rain-on-snow events, thereby enhancing the potential for future transferability to global applications.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.65]{conus_domain_orography.png}
\caption{\textbf{CONUS domain.} The figure shows the orography at 1 km resolution (in shades of green and brown) and the location of the 25 most populated cities (black dots) over the CONUS.}
\label{fig:conus_domain}
\end{figure}

\subsection{Observations: NOAA's Storm Event Database}
\label{flash_flood_focused_verification_rainfall_based_ff_DATA_SED}

The selection of the CONUS as the primary domain also lies on the high quality and comprehensiveness of NOAA's Storm Event Database. This database systematically records flash flood occurrences with high spatial and temporal resolution, including crucial metadata regarding impact severity, affected areas, and casualty information. The database's consistent reporting protocols and rigorous quality control procedures mitigate the observational uncertainty that typically plagues flash flood research \citep{Panwar_2020}.

The Storm Event Database serves as the US's official repository of severe weather records, including flash floods. Flash flood records began in 1996, with significant improvements in geographical precision implemented in 2007, when reporting transitioned from county-level to polygon-based representation, later extended to reports from 2001\footnote{\url{https://inside.nssl.noaa.gov/flash/database/}}.

The database covers the entire United States and territories with a spatial bounding box from 172.0°W to 65.0°E longitude and 18.0°N to 72.0°N latitude\footnote{\url{https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00510}}. Data collection involves a network of 123 Natioanl Weather Service Forecast Offices gathering information from emergency management officials, law enforcement, trained SKYWARN spotters, damage surveys, media reports, and public observations\footnote{\url{https://www.ncdc.noaa.gov/stormevents/faq.jsp}}.

The Storm Event provides extensive metadata for flash flood events. Among all the available keys, we considered in this study only the keys described in Table 3. Although professional meteorologists and hydrologists collect flash flood reports to include in the Severe Storm Database, any human augmented reporting system is subject to variations in population density, diurnal cycles of human activity, and more mundane transcription or memory errors that affect the timing and location of reports \citep{Barthold_2015}. Evidence of these issues has been found in assessments of FFG skill \citep{Clark_2014}, and it has been found that the distribution of SED's flash flood reports is affected by the distribution of human population \cite{Marjerison_2016}. 

\begin{figure}[htbp]
\centering
\includegraphics[scale = 0.85]{sed_ff_reports.png}
\caption{\textbf{Flash flood reports from NOAA's Storm Event Database.} Panel (a) displays the timeseries of flood reports from 1950 to 2024, distinguishing between all flood types (grey bars), only flash floods (dark red bars), and only flash flood reports with latitude/longitude coordinates (bright red bars) for the period 2001-2024. Panel (b) presents the spatial distribution of point flash flood report frequency per grid box for the period 2001-2024, shown as percentages (\%) across four geographical quadrants (North-West (NW, in shades of orange), South-West (SW, in shades of yellow), North-East (NE, in shades of green), South-East (SE, in shades of blue)) of the CONUS. The pie chart indicates the relative proportion of reports by region, with a total of 108,903 flash flood events reported in the considered period. Panel (c) illustrates the daily distribution of flash flood reports throughout 2021, showing both point reports (red bars) and gridded reports (black bars) accumulated over 24-hour periods ending at 00 UTC. The highlighted date with the blue circle (2nd September 2021) corresponds to the flash flood reports recorded during Storm Ida. Panel (d) maps the spatial distribution of flash flood reports, accumulated over the 24-hourly period ending at 00 UTC on 2 September 2021, displaying point reports with red dots and gridded reports with black dots. The zoomed-in box shows the reports recorded on the NE coast during Storm Ida.}
\label{fig:sed_ff_reports}
\end{figure}

In this study, we are using version 3.1 of the database, and we consider only reports over the CONUS that go from 2001 to 2024. 

\subsection{Rainfall forecasts: ERA5-ecPoint}
\label{flash_flood_focused_verification_rainfall_based_ff_DATA_ERA5_ecPoint}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{prob_tp_exceeding_1rp.png}
\caption{\textbf{Probability (\%) of exceeding the 1-year return period in ERA5-ecPoint.} Panel (a) displays probabilities in ERA5-ecPoint reanalysis for the valid time (VT) ending on 2021-09-02 at 00 UTC. Panels (b) to (f) represent the probabilities in ERA5-ecPoint forecasts for the same VT, but for forecasts at day 1 (t+0,t+24), day 2 (t+24,t+48), day 3 (t+48,t+72), day 4 (t+72,t+96), and day 5 (t+96,t+120), respectively.}
\label{fig:prob_tp_exceeding_1rp}
\end{figure}

ecPoint is a decision-tree-based statistical post-processing technique that transforms global grid-based forecasts into probabilistic point-scale forecasts \citep{Hewson_2021}. The post-processing technique aims to provide forecasts that mirror observations from rain gauges by addressing the two main factors affecting the performance of global NWP model outputs against point verification: systematic biases \citep{Lavers_2021} and lack of information on forecast sub-grid variability \citep{Göber_2008}. For each deterministic realisation or raw ensemble member of a probabilistic system, ecPoint generates an ensemble of X (e.g., 100) point-rainfall values based on the error distributions between forecasts and observations that vary according to different weather scenarios at the grid-box level. For example, when the model is on a grid-box, it primarily predicts large-scale rainfall with light winds. The raw model output tends to be representative of point rainfall totals within that grid-box, and ecPoint generates an ensemble with a smaller spread compared to the case of mainly convective rainfall with light winds. In the latter case, many points within the grid-box are expected to show zero rainfall, while large rainfall amounts may be observed at a few data points. The deterministic realisations of ERA5 reanalysis and forecasts were post-processed, creating a distribution of 100 point-rainfall totals, and distilled in 99 percentiles from 1st to 99th. ecPoint reanalysis and forecasts are provided in the same native grid of ERA5 (reduced Gaussian grid N320, \sim31 km), with forecasts up to day 5, and accumulation periods ending at 00 UTC.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{prob_tp_exceeding_50rp.png}
\caption{\textbf{Probability (\%) of exceeding the 50-year return period in ERA5-ecPoint.} Similar to Figure \ref{fig:prob_tp_exceeding_1rp}, but for probability of exceeding the 50-year return period.}
\label{fig:prob_tp_exceeding_50rp}
\end{figure}

Figure \ref{fig:prob_tp_exceeding_1rp} and \ref{fig:prob_tp_exceeding_50rp} show examples of the probabilities of exceeding the 1-year and 50-year return period in ERA5-ecPoint reanalysis and forecasts. It is worth noting that the ecPoint technique does not increase or decrease the probabilities of exceeding a certain threshold everywhere in the same way. In the example shown, the NE coast of the CONUS was affected by Storm Ida (a large-scale convective system). In this case, the probabilities of exceeding the 1-year and the 50-year return periods are uniformly large over a large area, indicating that this event might be fairly widespread. The signal remains fairly strong (with probabilities > 60\%) up to day 5 forecasts, showing a good predictability for this particular system. Over the SW quadrant of the CONUS, we can notice a large "bubbly" area of fairly large (> 20\%). The "bubbly" shape indicates the likelihood of very localised intense rainfall events from a small-scale convective system. The predictability of the event varies with increasing lead times, with the area being affected by intense rainfall decreasing for longer lead times, as well as the probabilities of experiencing intense rainfall. In this case, predicting the correct areas that may experience intense rainfall is more difficult as the predictions vary significantly over time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods: developing an objective flash-flood-focused 
verification framework for predictions of areas at risk of flash floods}
\label{flash_flood_focused_verification_rainfall_based_ff_METHODS}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{workflow_verif_framework.png}
\caption{\textbf{Workflow for flash-flood-focused verification framework.}}
\label{fig:workflow_verif_framework}
\end{figure}

\subsection{Data processing}

\subsubsection{Gridded observational fields}

To assess the predictions of areas at risk of flash floods, these predictions must be compared against ground-truth observations. The objective verification framework proposed in this thesis uses flash flood impact reports as ground truth. As indicated in section \ref{flash_flood_focused_verification_rainfall_based_ff_DATA_SED}, this thesis will focus on the use of the Storm Event Database over the CONUS. Notwithstanding, this method can be applied in other geographical regions where regional databases are available, such as over Europe (using the ESWD dataset) or a global domain (considering global datasets such as EM-DAT or DesInventar), while always acknowledging the biases that such impact databases might have. 

To compare forecasts and observations, the latter must be transformed into the format in which the forecasts are provided, i.e., gridded datasets aggregated over the considered accumulated outlook. This approach follows the methodology proposed by \citet{Tsonevsky_2018} for severe weather, and the proof-of-concept proposed by \citet{Pillosu_2024} for flash floods in Ecuador, and it is presented graphically in the scheme in Figure \ref{fig:workflow_verif_framework} ("Observations" column). As mentioned in section \ref{flash_flood_focused_verification_rainfall_based_ff_DATA_ERA5_ecPoint}, ERA5-ecPoint forecasts are provided in a reduced Gaussian grid N320, with a spatial resolution of \sim31 km at the equator. Hence, each flash flood report was aggregated over 24-hourly accumulation periods ending at 00 UTC. The outcome of this stage consists of a list of \textit{point} flash flood reports occurring between 00 UTC on January 1st and 00 UTC on January 2nd, between 00 UTC on January 2nd and 00 UTC on January 3rd, and so on. Each point report in a specific accumulation period must then be located on the ERA5 grid-box where the event occurred using the latitude/longitude coordinates of each impact report. In the Storm Event Database, the location of each flash flood event is provided by two latitude coordinates ("BEGIN\_LAT" and "END\_LAT") and two longitude coordinates ("BEGIN\_LON" and "END\_LON"). If BEGIN\_LAT = END\_LAT and BEGIN\_LON = END\_LON, the flash flood event is identified by a single point, and only the closest grid-box to that point is assigned the flash flood event using the "nearest grid-box" method. If BEGIN\_LAT \ne END\_LAT and BEGIN\_LON \ne END\_LON, a polygon is provided, and the flash flood event is assigned to all the grid-boxes within that polygon. It is worth noting that even when a polygon is provided, only one grid-box may contain the flash flood event if the radius of such a polygon is not larger than the resolution of the ERA5 grid-box, i.e., 31 km. This procedure creates \textit{gridded} fields of flash flood reports, where the values assigned to the grid-boxes indicate the number of flash flood reports that occurred within the grid-boxes: when a grid-box is assigned the value 0, it means it does not contain any flash flood reports for the considered accumulation period, while a grid-box with a value of 5 indicates that there are 5 point flash flood reports within that grid-box, for the considered accumulation period. During verification, all grid boxes with at least one flash flood report will be assigned a value of 1; otherwise, they will be assigned a value of 0.

Because of the large size of the ERA5 grid-boxes (i.e., 31 km) and the large accumulation periods for the flash flood outlooks (i.e. 24 hours), any additional expansion in the area or reporting times of individual reports is probably unnecessary to account for uncertainties in the location and reporting times as done in other studies \citep{Cavaiola_2024}. It is possible to envision remote scenarios in which a report is close enough to the edge of the ERA5 grid-box or the outlook accumulation period that a location or reporting time error in the database could cause the mislocation of such flash flood event, but this scenario has been established to be less than 0.1\% of the total number of the reports considered in this thesis, so no further actions will be considered here to account for spatio-temporal uncertainties in the location or reporting time in the Storm Event Database. 


\subsubsection{Forecast fields: definition of the verifying thresholds}

Given a forecast value at a grid-box, a flash flood event is considered a yes-event if the forecast exceeds a specific event threshold, and the grid-box is assigned the value 1; otherwise, the grid-box is assigned the value 0 (see Figure \ref{fig:workflow_verif_framework} - Rainfall-based forecasts).

For rainfall-based predictions of areas at risk of flash floods, rainfall-based thresholds must be used. These might be known for the region of interest, or must be computed using historical observational or modelled rainfall estimates. If point rainfall observations are available (e.g., rain gauges or radars), one can create the distribution of the observed flash-flood-triggering rainfall totals, from which to compute the verifying rainfall thresholds (VRT) values as specific percentiles from such distribution. The higher the percentile, the higher the magnitude of the VRT, and the higher the severity level of flash flood events considered in the objective verification analysis. This approach requires high-density rainfall observations, in both space and time, to capture the localised extreme rainfall totals that trigger flash floods \citep{Haiden_2016, RamosFilho_2021}. In the absence of a suitable observational network, the VRT values may be defined from gridded rainfall products such as reanalysis e.g., ERA5 \citep{Hersbach_2020}, reforecasts \citep{Hamill_2006b}, or blended rainfall observations provided on a grid such as MSWEP \citep{Beck_2019} or GPCP \citep{Adler_2018}. These datasets, however, tend to underestimate rainfall extremes because of their coarse spatial resolution \citep{Tapiador_2019}. In the absence of more suitable gridded datasets, \citet{Pillosu_2024} proposed a methodology to compute regional point rainfall climatologies using 1 year of short-range (day 1) ecPoint rainfall forecasts. This approach, however, has the disadvantage to use only 1 year of forecasts, which means that the resulting climatologies will inevitably be affected by the climatology of that specific year instead of the general climatology of the region that they are supposed to represent. Moreover, as this climatology requires flash flood observations to be defined, and such observations tend to have an even lower resolution than rainfall forecasts, there is a need to create climatologies for large domains, rather than on a grid-box scale. For example, \citet{Pillosu_2024} created only two verifying rainfall thresholds for Ecuador, one for the coastal and one for the Andean region. While this disaggregation is better than having one single value for an entire country, regional thresholds might not be able to capture flash-flood-triggering rainfall thresholds over different micro-climates. This would not allow an appropriate disaggregation of the performance of the flash flood forecasts in different regions, under different (hydro-meteorological) conditions. Hence, in this thesis, we propose the definition of the rainfall VRTs from a climatology built with ERA5-ecPoint rainfall estimates, as they have been shown to represent point-rainfall climatologies around the world reliably \citep{Pillosu_2025a} and they would allow us to develop a grid-box scale rainfall climatology to be used as thresholds for the rainfall-based predictions of areas at risk of flash floods. Hence, an ERA5-ecPoint rainfall climatology has been computed over the WMO-recommended 30-year period of 1991-2020 \citep{WMO_2017}.

\subsection{Objective verification}

The objective verification carried out in this thesis is based, instead, on verification scores computed from a probabilistic contingency table (Figure \ref{table:contingency_table}). Stationary observations (i.e., provided by instruments installed at a specific location, such as rain gauges or discharge gauges) provide timeseries of yes- and non-events recorded at the location where the instrument was installed. Thus, all four elements in the contingency table can be quantified. Non-stationary observations (i.e., impact reports) record only yes-events at the location where the event occurred. As a result, it is impossible to answer the question "if there are no reports at a location, is it because an event happened but nobody reported it, or because there was no event to report?". Some studies using impact reports as ground truth verify only yes-events with the caveat that only quadrant I (i.e. hits) and III (i.e. misses) of the contingency table can be populated \citep{Robbins_2018}. In this thesis, the assumption suggested by \citet{Tsonevsky_2018} and \citet{Pillosu_2024} of assuming non-reports as non-events was followed because it allows for the population of all quadrants of the contingency table. This approach inherently inflates the false alarm rate, as locations where events may have occurred but were unreported may be classified as misses or correct negatives. Given the constraints of impact-based reports, this methodology provides a systematic framework for evaluation of predictions of areas at risk of flash floods, albeit one that may present a pessimistic view of forecast skill.

\begin{table}[htbp]
\centering
\captionof{table}{\textbf{Example of a 2x2 contingency table}. The table defines the four quadrants constituting a 2x2 contingency table.}
\includegraphics[width=\textwidth]{contingency_table.png}
\label{table:contingency_table}
\end{table} 

The probabilistic contingency tables are built by examining overlapping grid-boxes in corresponding observational and forecast fields. When both grid-boxes are assigned a value of 1 or 0, they count as a hit or a correct negative, respectively. When a grid-box in the observational field is assigned a value of 1, and the corresponding grid-box in the forecast field is assigned a value of 0, it counts as a miss. It counts as a false alarm if the opposite occurs. 

\subsection{Properties of probabilistic forecasts}
Reliability and discrimination ability are desirable properties of ensemble forecasts, and both are defined against a verifying threshold \citep{Jolliffe_2012, Wilks_2020}. Reliability measures whether the chosen verifying threshold is predicted with probabilities that mirror the frequency with which the considered event is observed. Discrimination measures the forecasts' ability to distinguish situations that lead to events exceeding the verifying threshold from those that do not, appraising the existence of a signal in forecasts when an event materialises. In this thesis, we consider two types of scores to analyse reliability and discrimination ability: summary and breakdown scores. Summary scores show the overall reliability and discrimination ability of the forecasts, while the breakdown scores provide detailed insights into how reliability and discrimination ability relate to the full distribution of probabilities. 

\subsubsection{Reliability}
To assess the overall reliability of the forecasts, we will consider the frequency bias (FB) to evaluate the overall reliability of the predictions of areas at risk of flash floods. The frequency bias was determined by dividing the total number of yes-events in the forecasts by the total number of yes-events in the observations. FB values range from 0 to $+\infty$. FB = 1 indicates perfect calibration, while scores greater or smaller than 1 indicate, respectively, over- and under-prediction of the observed yes-events. It is worth noting that FB measure the overall ratio of forecast events to observed events and is not a measure of forecast skill. As such, it can provide a score of 1 when there are compensating errors. Moreover, the FB might show large overestimations if the observed event is heavily underreported, as in our case.

To breakdown the reliability of the forecasts over the full distribution of probabilities, reliability diagrams are used. They plot the relative forecasts probability of an event against its corresponding relative observational frequency, indicating how reliable the forecast probabilities are at different classes. For perfect forecasts, when the forecasts show x\% probability of occurrence, observations should meet the criteria x\% of the time, so that the reliability curve lies on the diagonal. If the reliability diagram is above the diagonal for a specific forecast probability, those forecasts are under-predicting the likelihood of observing a yes-event. If it lies below the diagonal, there is over-prediction. When analysing reliability diagrams, it is also important to know the frequency distribution of forecasts issued with specific probabilities. For example, the small probability thresholds (within the red box in the figure example) are the most important when considering high verifying thresholds because the sample of forecasts exceeding the verifying threshold with high probabilities is rather small. For this reason, reliability diagrams should always be accompanied by sharpness diagrams, which plot the absolute frequency of forecasts at different probability thresholds. 


\subsubsection{Discrimination ability}

Relative Operating Characteristic (ROC) curves are built from 2x2 contingency tables (Table \ref{table:contingency_table}), quantifying hits (H) misses (M), false alarms (FA), and correct negatives (CN). Hit rates (HR) and false alarm rates (FAR) are computed, respectively, from equations \ref{eq:hr} and \ref{eq:far}:

\begin{equation}
\mathrm{HR} = \frac{\mathrm{H}}{\mathrm{H} + \mathrm{M}}\quad[\text{values between }0\text{ and }1]
\label{eq:hr}
\end{equation}

\begin{equation}
\mathrm{FAR} = \frac{\mathrm{FA}}{\mathrm{FA} + \mathrm{CN}}\quad[\text{values between }0\text{ and }1]
\label{eq:far}
\end{equation}

HRs are mapped (Y-axis) against FARs (X-axis) in a unit square. The form of the ROC curve illustrates how HRs vary with FARs as one systematically lowers the threshold probability at which it is assumed that an event has been technically forecast to happen (i.e., with a 100\% probability in the bottom left corner to a 0\% probability at the top right corner). The values of the geometrical area under the ROC curve (AROC) provide a summary measure of the discrimination ability across all probability thresholds. Perfect discrimination ability is obtained when only HRs grow and FARs remain zero. It is represented by an ROC curve that rises along the Y-axis from the bottom left corner of the unit square to the top left corner and moves straight to the top right corner. In this case, the AROC equals 1. If HRs and FARs grow at the same rate, the forecasts may appear to lack discrimination ability, as they perform similarly to a climatological forecast or due to a limited number of issued forecasts exceeding the VRT. In this case, the ROC curve lies along the diagonal, and AROC equals 0.5. 

How ROC curves and AROCs are computed can impact the interpretation of forecasts discrimination ability. For rainfall-based predictions, the ROC curves will be built for incremental decision thresholds that are materially assessable from the real ensemble configuration. In this way, we can estimate the "real" forecast discrimination ability \citep{Wilks_2020}. Probability thresholds are determined by considering the full discretisation ability in the ensemble (e.g., 99 members in the case of ERA5-ecPoint). This ensures that the ROC curves are as complete as possible \citep{Bouallegue_2022}. The number of thresholds corresponds, therefore, to the number of members exceeding the verifying rainfall threshold, so that for an ensemble of size M, maximum discretisation is achieved by M+1 probability thresholds (i.e., 0, 1/M, 2/M, ...., M/M=1). The ROC curve is then built by straight segments joining successive points. It is then completed by joining that last meaningful point with a straight line in the top right corner of the unit square. For rare events, the points of a ROC curve cluster in the graph's bottom left corner and completing the ROC with a straight line might give the impression that part of the ROC curve is missing \citep{Casati_2008}. How much the curve appears incomplete depends on the ensemble size and the base rate of the event. The area under the ROC curve (AROC) will be computed using a trapezoidal approximation by adding the areas of single trapeziums formed by the straight lines between consecutive points in the ROC curve \citep{Bouallegue_2022}. 

From what written before, the ROC curves represent the breakdown measure of discrimination ability as it will be possible to examine the values of HRs and FARs at different decision (probability) thresholds. AROC will represent instead the overall measure of discrimination ability.

%%%%%%%%%%%%%%%%%
\section{Results}
\label{flash_flood_focused_verification_rainfall_based_ff_RESULTS}


\subsection{Overall verification scores: frequency bias and area under the ROC curve}

The \marginpara{Overall reliability: frequency bias (FB)} frequency bias (Figure \ref{figu:rainfall_based_ff_verif_overall_scores}a) reveals systematic overprediction across all return periods, with values relatively stable across all lead times. The frequency bias is larger - around 70 - for less severe rainfall events (e.g., 1-year return period). When considering a slightly more severe event (e.g., 5-year return period), the frequency reduces to a third - around 20, and it becomes remarkably smaller - around 2 - for very extreme rainfall events (e.g. 50-year and 100-year return period, see inset in Figure \ref{figu:rainfall_based_ff_verif_overall_scores}a). 







, with values ranging from approximately 10 for the 100-year return period to over 70 for the 1-year return period in the reanalysis. This overprediction pattern persists across all lead times, though with notable variations. For extreme events (return periods ≥20 years), the frequency bias stabilizes between 2 and 10, suggesting more reliable prediction of severe events. The inset panel highlights that perfect bias (FB=1) is never achieved for any configuration, with the closest values occurring for the highest return periods.




All \marginpara{Overall discrimination ability: area under the ROC curve (AUC-AROC)} forecasts for rainfall events exceeding the 1-year return period threshold (Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}) exhibit a discrimination ability superior to random chance, as the curves are above the diagonal reference line. A systematic degradation in discrimination ability is observed with increasing lead time, with the Area Under the ROC Curve (AROC) values ranging from 0.675 for the short-range forecasts (Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}a) to 0.612 (\sim9\% reduction) for t+120 (day 5, Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}f). Despite such a reduction, the forecasts show a good discrimination ability throughout the forecast horizon. Day 1 forecasts (t+24) show a higher discrimination ability than the short-range forecasts, and only from day 2 forecasts (t+48), the discrimination ability of the long-range forecasts goes below that of the reanalysis. The relatively narrow confidence intervals (at 99\% confidence level) suggest that the differences in skill between forecast configurations are statistically meaningful at the considered confidence level.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_overall_scores.png}
\caption{\textbf{Overall verification scores for the rainfall-based forecasts of areas at risk of flash flood.} Panel (a) shows the frequency bias (solid lines) for 1-year (in red), 5-year (in purple), 10-year (in light green), 20-year (in cyan), 50-year (in blue), and 100-year return period (in green). The corresponding shaded areas represent the confidence intervals at 99\% confidence level. The inset box contains a zoomed-in version of the panel to show better the frequency bias values close to 1 (representing perfect bias). Panel (b) shows the area under the ROC curve.}
\label{fig:rainfall_based_ff_verif_overall_scores}
\end{figure}





\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_1rp.png}
\caption{\textbf{ROC curves for tp >= 1-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Panel (a) shows the ROC curve (blue solid line) for the short-range predictions together with the confidence intervals (blue shaded area) at 99\% confidence level. Panels (b) to (f) refer to the long-range forecasts, for accumulation periods ending in t+24, t+48, t+72, t+96, and t+120, respectively. The pink dots refer to the probability threshold at which the frequency bias has the closest value to 1 (i.e., perfectly reliable forecast), while the orange dot shows the value of the frequency bias for the lowest probability threshold available in ERA5-ecPoint (i.e., the 99th percentile).}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}
\end{figure}

The pink dot in Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}a shows that perfect reliability (i.e. frequency bias equal to 1) is reach for probabilities <= 23\%. For the long-range forecasts, the probability thresholds at which perfect reliability is achieved is compatible to the short-range, being 27\% for all the lead times except t+120 which is 26\%. The frequency bias for the lowest probability threshold (i.e. 99th percentile or probability threshold equal to 1\%) in the short-range forecasts equals to 28. The frequency biases for the long-range forecasts are similar, falling between 31 and 33. 

Similar results are obtained for the 5-, 10-, 20-, 50-, and 100-year return periods.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_5rp.png}
\caption{\textbf{ROC curves for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_5rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_10rp.png}
\caption{\textbf{ROC curves for tp >= 10-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_10rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_20rp.png}
\caption{\textbf{ROC curves for tp >= 20-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_20rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_50rp.png}
\caption{\textbf{ROC curves for tp >= 50-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_50rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{chapter_05/figures/rainfall_based_ff_verif_breakdown_scores_roc_100rp.png}
\caption{\textbf{ROC curves for tp >= 100-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_roc_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_roc_100rp}
\end{figure}


\subsection{Reliability}

All forecasts for rainfall events exceeding the 1-year return period threshold (Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}) exhibit a systematic overprediction across all lead times, as shown by the reliability diagram being below the diagonal line. This indicates that when the model predicts a given probability, the observed frequency of flash flood events is consistently lower. For example, when the forecasts indicate a 50\% chance of having a flash flood event, the observed frequency ranges from \sim10\% in the short-range forecasts (Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}a), and between 10\% (for t+24, Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}b) and 2\% (t+120, Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}f) in the long-range forecasts. As seen in the ROC curves, the confidence intervals at 99\% are fairly narrow, suggesting that the differences between the reliability diagrams at different lead times are significant at the considered confidence level. However, the confidence levels increase with increasing forecast probabilities. As seen in the corresponding sharpness diagrams (inset boxes in all panels of Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}), such widening of the confidence intervals is likely due to the low number of forecasts issued with probabilities higher than ~25\% (when the total number of instances lies below 1000 samples). Such predominance of low probability forecasts suggests that the model (ERA5-ecPoint) rarely expresses high confidence in extreme event occurrence. A notable characteristic of all the reliability diagrams in the Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp} is the sharp increase in observed frequency for the highest probability bins (i.e. 80\% to 100\%). This steep rise suggests that when the model does issue high probability forecasts, these correspond to genuinely extreme events, though such forecasts remain infrequent.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp.png}
\caption{\textbf{Reliability diagrams for tp >= 1-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Panel (a) shows the reliability diagram (blue solid line) for the short-range predictions together with the confidence intervals (blue shaded area) at 99\% confidence level. Panels (b) to (f) refer to the long-range forecasts for accumulation periods ending in t+24, t+48, t+72, t+96, and t+120, respectively. The inset boxes show the corresponding sharpness diagrams.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}
\end{figure}

The temporal evolution from Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}a to f reveals subtle changes in the forecasts' reliability characteristics with increasing lead time. Whilst the general pattern of overprediction persists, from day 2 forecasts (t+48) results more squashed into a flat line over very small observed frequencies, indicating that even though the model issues forecasts with high probabilities of exceeding the 1-year return period at longer lead times with a similar frequency of the short-range forecasts and the day 1 (t+24) long-range forecast, such forecasts do not necessarily correspond to an observed flash flood event.

The reliability diagrams get closer to the diagonal line (indicating perfect bias) as we increase the rainfall threshold to identify the flash flood events. Perfect reliability is observed for rainfall events exceeding the 10-, 20-year, and 50-year return period with probabilities below 5\% at day 1 forecast (t+24) for the 10-, 20-year return period, and 10\% for the 50-year return period.

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_5rp.png}
\caption{\textbf{Reliability diagrams for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_5rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_10rp.png}
\caption{\textbf{Reliability diagrams for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_10rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_20rp.png}
\caption{\textbf{Reliability diagrams for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_20rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_50rp.png}
\caption{\textbf{Reliability diagrams for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_50rp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{rainfall_based_ff_verif_breakdown_scores_rel_diag_100rp.png}
\caption{\textbf{Reliability diagrams for tp >= 5-year return period for the rainfall-based forecasts of areas at risk of flash floods built with ERA5-ecPoint.} Similar to Figure \ref{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_1rp}.}
\label{fig:rainfall_based_ff_verif_breakdown_scores_rel_diag_100rp}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%
\section{Case Study: Storm Ida}
\label{flash_flood_focused_verification_rainfall_based_ff_CASE_STUDY}

Tropical \marginpara{Storm Ida: synoptic history } Storm Ida formed on 23 August 2021 in the western Caribbean Sea, southwest of Jamaica. It first hit western Cuba on 27 August as a Category 1 Hurricane (Figure \ref{}). It then travelled northwestward towards the Gulf of Mexico, where it intensified rapidly due to sea surface temperatures of nearly 30°C. Ida made landfall on the Louisiana coast near Port Fourchon on 29 August 2021 at 16:55 CDT (Central Daylight Time, UTC-05:00 - 16:55 UTC) as a Category 4 Hurricane, with maximum sustained winds of 150 mph. The cyclone’s intensity steadily decreased as it moved inland, and it weakened to a tropical storm before the centre moved into southwestern Mississippi between 06:00 and 12:00 UTC on 30 August 2021. Ida then turned northeastward as it moved around the western end of the subtropical ridge, with the centre passing just west of Jackson, Mississippi, around 18:00 UTC. Soon thereafter, the cyclone weakened to a tropical depression as it moved into northeastern Mississippi. The system then accelerated northeastward across northwestern Alabama, central and eastern Tennessee, and portions of Kentucky and Virginia before reaching southern West Virginia near 1200 UTC 1 September. Ida began extratropical transition as it moved through the Tennessee Valley, and the system became an extratropical low as it moved over West Virginia later that day. Once it became extratropical, Ida moved east-northeastward in the mid-latitude westerly flow through West Virginia, northern Virginia, and central Maryland to southeastern Pennsylvania by 0000 UTC 2 September. At that time, the system acquired gale-force winds over the Atlantic east of the center. A continued east-northeastward motion brought the center across northern New Jersey and into the Atlantic just south of Long Island, New York, to near Nantucket, Massachusetts by 1200 UTC that day. The low then turned northeastward and strengthened a little, reaching western Nova Scotia late on 2 September and moving into the Gulf of St. Lawrence on 3 September. This was followed by a cyclonic loop over the Gulf of St. Lawrence on 3–4 September while the low maintained maximum winds of 40–45 kt. The low degenerated to a trough late on 4 September as a new mid-latitude low formed to the east \citep{Beven_2022}.

As \marginpara{Storm Ida: rainfall and flooding} a tropical cyclone, Ida produced widespread heavy rain along portions of the northern Gulf coast states northward and eastward into the Tennessee Valley. Rainfall totals of more than 10 inches occurred over portions of southeastern Louisiana, southeastern Mississippi, and southwestern Alabama, with a maximum storm total of 15.04 inches (382 mm) at Ponchatoula, Louisiana, and a storm total of 13.65 inches (346.7 mm) near Kiln, Mississippi. Rainfall totals of 5–9 inches, with locally higher amounts, occurred over much of eastern Mississippi, southwestern Alabama, and the western Florida Panhandle. Rainfall totals of 3–5 inches occurred over other portions of western and northern Alabama, northwestern Georgia, eastern Tennessee, and central and eastern Kentucky. These rains produced freshwater flooding, especially along the Tangipahoa, Tchefuncte, Tickfaw, and Bogue Falaya Rivers in southeastern Louisiana and the Tchoutacabouffa, Biloxi, Wolf, and Jourdan Rivers in southeastern Mississippi. When Ida became extratropical, a swath of heavy rains with local rainfall rates near or above 3 inches per hour (76.2 mm) developed north of the centre and affected a long area extending from northern West Virginia, across western Maryland, southeastern Pennsylvania, northern New Jersey, southeastern New York, Connecticut, and Rhode Island to southeastern Massachusetts, including the New York City metropolitan area. Maximum storm total rainfalls in these areas include 10.10 inches (256.54) at Downingtown, Pennsylvania, 10.06 inches (255.5 mm) at Manville, New Jersey, 9.64 inches (244.9 mm) at Staten Island, New York, 9.22 inches at Uncasville, Connecticut, and 8.16 inches near Frederick, Maryland. The extreme rainfall rates and heavy rainfall caused major freshwater flooding in these areas, including deadly and damaging flash flooding and urban flooding across portions of the New York City metropolitan area and northern New Jersey. Ida caused the fifth-wettest day in NYC history\footnote{https://www.ncei.noaa.gov/access/billions/dcmi.pdf}.

Nearly \marginpara{Storm Ida: impacts} all of southeast Louisiana lost power, leaving more than 1 million customers without electricity as the storm tracked northward through the state and into Mississippi. Even days later, on September 3, more than 900,000 remained without power. The storm devastated the Louisiana coast. Virtually every house on the barrier reef of Grand Isle was damaged, with many destroyed. Although the damage was immense, the good news was that the flood barriers in New Orleans held, so the city avoided the deadly flooding that had occurred during Hurricane Katrina in 2005. The band of torrential rains that stretched across New Jersey into the New York City area and Connecticut that Wednesday night triggered a rare flooding emergency. Central Park recorded an unprecedented one-hour rainfall of 3.15 inches (78.75 mm/1h). Calendar day rainfall totalled 7.13 inches (181.1 mm/24h). Newark saw 8.44 inches (214.4 mm/24h), making it its wettest day on record. Flooding was so severe and sudden that basement apartment dwellers in New York City were overwhelmed by the floodwaters \citep{LeComte_2022}. Ida was blamed for 96 deaths total and cost \$75 billion, making it the 6th most costly storm to impact the US\footnote{https://www.ncei.noaa.gov/access/billions/dcmi.pdf}.


% caption:
% National Hurricane Center best track of Hurricane Ida (https://www.weather.gov/lch/2021Ida)



%%%%%%%%%%%%%%%%%%%%%
\section{Discussions}
\label{flash_flood_focused_verification_rainfall_based_ff_DISCUSSIONS}


For \marginpara{Reliability and discrimination ability of the rainfall-based predictions of areas at risk of flash floods} high-frequency events (e.g., 1-year return period), the high frequency bias values indicate that ERA5-ecPoint forecasts identify potential flash flood areas up to 70 times more frequently than such events are observed in the Storm Event Database. In marked contrast, extreme events (50-100 year return periods) demonstrate near-optimal frequency bias values between 2 and 3. These results may have two interpretations. The first one concerns the quality of the forecasts, indicating that rainfall-based forecasts of areas at risk of flash flood based on rainfall forecasts exceeding at least the 50-year return period are the ones that should be considered for the prediction of areas at risk of flash floods as they are the ones providing near-optimal reliability. The second interpretation concerns the type of flash flood events recorded in the database, i.e., mostly extreme flash flood events, generated by rainfall events exceeding the 50-year return period, are recorded in the database. It is not possible for us to dissentangle from the information at hand, which of the two interpretations might be the correct one. 



achieve near-optimal reliability for severe events despite their rarity. 








Although \marginpara{Interpreting verification metrics with sparse observational coverage} the Storm Event Database correspond to one of the best attempts to build a comprehensive historical record of flash flood events (in the US) , event underreporting still needs to be addressed to provide a more comprehensive assessment of forecast performance. In alignment with previous studies attempting to use impact-based observations to estimate forecast performance \citep{Hitchens_2013, Robbins_2018, Mitheu_2023, Mitheu_2025}, the verification of the rainfall forecasts against underreported flash flood events can lead to an underestimation of forecast skill and undermine the confidence in the forecasts, causing the dismissal of valuable predictions crucial for preparedness actions. It is, therefore, required to read beyond the actual numbers of verification results, and read them in a critical, although subjective, way. For example, although the FB is far larger than 1 (that would mean the forecasts overestimate substantially the areas at risk of flash floods), the counts of yes-events in the forecast and the observations show there is a good correspondence between wet/dry conditions (meaning the forecasts can identify fairly well which areas are at risk of flash floods) and that the high FB might be due mainly to the low spatial coverage of the reports. Techniques such as assigning 1s (i.e., yes-events) to adjacent grid-boxes to those already containing flood reports could help to reduce the FB. We argue, however, that this method is more appropriate for forecasts provided on higher resolution grids because rainfall might not be predicted at the right location and/or the flash flood event might extend to adjacent grid-boxes. Since forecasts are here provided on a grid at 31 km, if a flash-flood-triggering rainfall event is predicted within a grid-box, it is reasonable to think that there is a good chance to see the flash flood within that grid-box (unless the event happens on the grid-box boundary, but this is an exception whose handling goes beyond the scope of this analysis). It is, therefore, argue here that, when verifying global NWP forecasts, improvements in the observational spatial coverage are the way to reduce the number of “false” false alarms, which for this type of analysis (using non-standard observations) is the biggest and most difficult problem to address \citep{Marsigli_2021}.

This \marginpara{Adequacy of the impact observations for verifying flash floods} study demonstrates that enhanced flash flood report databases are instrumental in assessing the performance of rainfall forecasts for flash flood prediction. The improved and more spatio-temporal coverage of flash flood reports in the Storm Event Database enabled an in-depth, long-term assessment that would have been unfeasible with other databases, such as EM-DAT, due to their poorer spatio-temporal coverage. For these reasons, flash flood verification in the past was primarily based on case studies or at catchment level, as more detailed information is available for single events \citep{Gaume_2009, Gaume_2016}. While a case-study-based verification approach or at catchment level is invaluable in understanding how forecasts predict flash flood events, provided enough observations are available, the results may not hold for other events due to the focused nature of the analysis. Alternatively, by leveraging the higher quality and spatial coverage of rainfall observations, researchers can use them to assess the performance of rainfall forecasts in predicting flash floods. However, as seen in this study, the results of these two verification analyses are different. ecPoint almost always performs better than ENS in predicting extreme (localised) rainfall \citep{Gascón_2024, Hemri_2022, Hewson_2021}. However, in the binary prediction of whether an area was affected by a flash flood or not (yes- or no-event), the verification results are more nuanced. Failing to consider these two cases separately would do a disservice to the assessment of how well raw ENS forecasts can predict areas at risk of flash floods. Thus, this study highlights the importance of enhancing flash flood report databases to assess the performance of rainfall forecasts for flash flood prediction in more detail, thereby contributing significantly to more effective disaster preparedness and risk management strategies.

%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
\label{flash_flood_focused_verification_rainfall_based_ff_CONCLUSIONS}

In this chapter, the performance of post-processed global reanalysis (ERA5) and forecasts (ERA5-Forecasts), post-processed with the ecPoint methodology, in predicting areas at risk of flash floods. 

Overall discrimination ability, measured by the AUC-AROC, and the overall reliability, measured by the frequency bias, remain fairly constant over lead time. Moreover, the performance of the forecasts at day 1 is comparable to that obtained from the analysis. While the AUC-ROC for very extreme events (e.g., events with a return period > 20 years) are very close to 0.5, the ROC curves show that the post-processed versions of the ERA5 reanalysis and forecasts identify the yes-events with barely any false alarms at all lead times. Moreover, the reliability diagrams indicate a very good reliability for these extreme events. Hence, the squashed ROC curve toward the bottom left corner of the unit square and the corresponding small AUC-ROC may be due to the low base rate of these extreme events.

Another important point outlined in this study regards the quality of flash flood impact report databases. While the quality of the Storm Event Database is much bigger than other similar databases (e.g., EM-DAT), this chapter discussed the impacts in the objective verification of areas at risk of flash floods of a still large event underreporting. While the enhanced quality of the database used in this analysis allowed the authors to conduct an in-depth, long-term verification analysis, there is still work to do to better assess forecast performance to support preparedness and action during decision-making processes. The authors of this study suggest that more resources should be spent in the development of more flood databases like the Storm Event Database as they incorporate invaluable details on the type of flood that can be used to target verification and ofrecast development efforts. 

At the same time, research efforts should be focused in addressing the uncertainties around the spatial coverage of this type of observations to provide clearer guidance on forecast performance.